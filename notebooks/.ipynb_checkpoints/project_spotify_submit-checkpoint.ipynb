{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "import json\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "\n",
    "# Spotify\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "client_credentials_manager = SpotifyClientCredentials(client_id='e6ff82a6418a4191a5b3a95622faf5dd', client_secret='a37b632dc07d4136902fa95ec56281d3')\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n",
    "\n",
    "# Genius\n",
    "Genius_TOKEN = 'C2ow8dBpT2W5ORhiqaiz8ht8zLs9UzjFJQS5fwsmkRwWZyj8Wi1dA37FXYjScYuu'\n",
    "\n",
    "import re\n",
    "\n",
    "pd.set_option('display.width', 1500) \n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. [Read the data from Million Playlist Dataset]\n",
    "## 1) Incorporate playlist's metadata to the columns of each track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FILES = 2\n",
    "\n",
    "playlists_df = pd.DataFrame() \n",
    "\n",
    "for file_num in range(NUM_FILES):\n",
    "    FILENAME = 'Data/mpd.v1/data/mpd.slice.' + str((file_num)*1000) + '-' + str((file_num+1)*1000-1) + '.json'\n",
    "#     print(FILENAME)\n",
    "    \n",
    "    with open(FILENAME, \"r\") as fd:\n",
    "        tempdata = json.load(fd)\n",
    "    \n",
    "    print(f'[File {file_num}]')\n",
    "    \n",
    "    NUM_PLAYLISTS_PER_FILE = 10 # len(tempdata['playlists'])\n",
    "    \n",
    "    for playlist_num in range(len(tempdata['playlists'])):\n",
    "        if playlist_num < NUM_PLAYLISTS_PER_FILE:\n",
    "            single_playlist_df = pd.DataFrame(tempdata['playlists'][playlist_num]['tracks'])\n",
    "\n",
    "            for playlist_key in tempdata['playlists'][playlist_num].keys():\n",
    "                if playlist_key != 'tracks':\n",
    "                    if (playlist_key != 'duration_ms'):\n",
    "                        single_playlist_df[playlist_key] = tempdata['playlists'][playlist_num][playlist_key]\n",
    "                    else:\n",
    "                        single_playlist_df['playlist_duration_ms'] = tempdata['playlists'][playlist_num][playlist_key]\n",
    "\n",
    "            playlists_df = playlists_df.append(single_playlist_df, sort = True)\n",
    "\n",
    "            out = (playlist_num+1) * 1. / NUM_PLAYLISTS_PER_FILE * 100\n",
    "            sys.stdout.write(\"\\r %.1f %%\" % out)\n",
    "            sys.stdout.flush()\n",
    "    \n",
    "    print(f'\\n\\n{(file_num+1)/NUM_FILES*100:.2f}% Completed...\\n')\n",
    "\n",
    "playlists_df = playlists_df.set_index(np.arange(len(playlists_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Notes]**\n",
    "- Each file in our database contains 1000 playlists, and our database (Million Playlist Dataset) has total 1000 files.\n",
    "    - Thus, in total $1000 \\times 1000 = 10^6$ (million) playlist. <br>\n",
    "    <br>\n",
    "- Each playlist contains different number of tracks. It varies from 10 to 250 roughly.\n",
    "    - Let's say the average number of tracks per playlist is 50. Then, we have 50 million tracks in our database. <br>\n",
    "    <br>\n",
    "- We want to get more information of each track by doing web scraping. <br>\n",
    "    - e.g. lyrics, bpm (beats per minute), genre, etc.\n",
    "    - In this case, we need to request the contents (information of the track) from the website. <br>\n",
    "    - However, we cannot send request to the server too fast, because it will overload the server and server will be down. <br>\n",
    "    Or, the server will block us by regarding our requests as DDos (Distributed Denial of Service) attack. <br>\n",
    "    <br>\n",
    "    - Let's say we have a 1 second break for every request. Then, in order to get all the information, we need 50 million seconds, which takes 578 days... <br>\n",
    "    - Thus, for project milestone 3, we used only small part of the whole dataset. <br>\n",
    "    We read only 2 files and 10 playlists per file. <br>\n",
    "    Thus, in total, 20 playlists and 1566 tracks were explored. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[The basic EDA workflow]**\n",
    "1. **Build** a DataFrame from the data <br>\n",
    "<br>\n",
    "2. **Clean** the DataFrame from the data\n",
    "    - Each row describes a single object\n",
    "        - Each row : single track (in our data set)\n",
    "    - Each column describes a property of that object\n",
    "        - Each column : album name, artist name, duration (ms), name of playlist containing this track, ...\n",
    "    - Columns are numeric whenever appropriate\n",
    "        - Since each observation is a single track, many columns are text (string type; e.g. artist name, album name, track name, ...)\n",
    "    - Columns contain atomic properties that cannot be further decomposed. <br>\n",
    "        - Some columns are url of many different things (e.g. album URL, artist URL, track URL, ...) that can be furether decompose.\n",
    "        - **Thus, we can perform web scrapping to further decompose those columns.** <br>\n",
    "    <br>\n",
    "3. Explore **global perperties**.\n",
    "    - Histograms, scatter plots, and aggregation functions to summarize the data <br>\n",
    "    <br>\n",
    "4. Explore **group properties**.\n",
    "    - Groupby, queries, and small multiples <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Handle the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('[Check the number of missing values.]\\n')\n",
    "print(np.sum(playlists_df.isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_values = {'description' : ''}\n",
    "\n",
    "playlists_df = playlists_df.fillna(value = na_values)\n",
    "\n",
    "display(playlists_df.head())\n",
    "display(playlists_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Size of playlists_df : {playlists_df.shape}\\n')\n",
    "print('[Check the number of missing values.]\\n')\n",
    "print(np.sum(playlists_df.isna()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. [Web scraping]\n",
    "## 1) Get lyrics of each track\n",
    "### [1] https://genius.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_song_info(song_title, artist_name, album_name, token : str, option : int = -1):\n",
    "    base_url = 'https://api.genius.com'\n",
    "    headers = {'Authorization': 'Bearer ' + token}\n",
    "    search_url = base_url + '/search'\n",
    "    \n",
    "    if (option == 0):\n",
    "        data = {'q': song_title}\n",
    "    elif (option == 1):\n",
    "        data = {'q': album_name}\n",
    "    else:\n",
    "        data = {'q': song_title + ' ' + artist_name}\n",
    "    \n",
    "    response = requests.get(search_url, data=data, headers=headers)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_song_url(url):\n",
    "    page = requests.get(url)\n",
    "    html = BeautifulSoup(page.text, 'html.parser')\n",
    "    lyrics = html.find('div', class_='lyrics').get_text()\n",
    "\n",
    "    return lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2] http://lyrics.wikia.com/wiki/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_soup(filename: str) -> BeautifulSoup: \n",
    "#     '''Open the file and convert into a BS object. \n",
    "       \n",
    "#        Args:\n",
    "#            filename: A string name of the file.\n",
    "       \n",
    "#        Returns:\n",
    "#            A BS object containing the HTML page ready to be parsed.\n",
    "#     '''\n",
    "#     # your code here\n",
    "#     with open(filename) as fdr:\n",
    "#         data = fdr.read()\n",
    "#     return BeautifulSoup(data,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_track_name(trackname_arg: str) -> str:\n",
    "    '''\n",
    "    Input\n",
    "        trackname_arg : raw track name\n",
    "    Output\n",
    "        trackname : processed track name\n",
    "    '''\n",
    "    tempname = trackname_arg.replace('(','-').split('-')[0].strip()\n",
    "    trackname = \"_\".join(tempname.split(' '))\n",
    "    \n",
    "    return trackname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_lyric_page(bs_arg: BeautifulSoup, is_genius_arg : bool = True) -> str:\n",
    "    '''\n",
    "    Input\n",
    "        bs_arg : A BS object containing the HTML page (single track) ready to be parsed.\n",
    "        is_genius_arg : True if web scraping from genius website\n",
    "                        False if web scraping from lyrics.wiki website\n",
    "    Returns:\n",
    "        lyric : returning the lyric of the track\n",
    "    '''\n",
    "    \n",
    "    if is_genius_arg:\n",
    "        tag_str = 'div.lyrics'\n",
    "    else:\n",
    "        tag_str = 'div.lyricbox'\n",
    "    \n",
    "    if len(bs_arg.select(tag_str)) > 1:\n",
    "        print(\"WARNING: multiple tags\")\n",
    "        return bs_arg.select(tag_str)[0].text.strip()\n",
    "    else:\n",
    "        try:\n",
    "            return bs_arg.select(tag_str)[0].text.strip()\n",
    "        except IndexError:\n",
    "            print(\"WARNING: no tag\")\n",
    "            return 'N/A'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3] Seach the lyric in Genius & Lyrics.Wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(str_arg: str) -> str:\n",
    "    '''\n",
    "    Input\n",
    "        str_arg : string to be compared\n",
    "    Output\n",
    "        cleaned_str : lowercase string with only alphabetic and numeric letters\n",
    "    '''\n",
    "    cleaned_str = re.sub('[^A-Za-z0-9]', '', str_arg).lower()\n",
    "        \n",
    "    return cleaned_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_match(str1_arg: str, str2_arg: str) -> bool:\n",
    "    '''\n",
    "    Input\n",
    "        str1_arg : 1st string\n",
    "        str2_arg : 2nd string\n",
    "    Output\n",
    "        partial_match : True if there is a partial match\n",
    "    '''\n",
    "    partial_match = False\n",
    "    \n",
    "    if clean_str(str1_arg) in clean_str(str2_arg.replace('(','-').split('-')[0]):\n",
    "        partial_match = True\n",
    "    elif clean_str(str2_arg.replace('(','-').split('-')[0]) in clean_str(str1_arg):\n",
    "        partial_match = True\n",
    "    \n",
    "    if clean_str(str1_arg) in clean_str(str2_arg):\n",
    "        partial_match = True\n",
    "    elif clean_str(str2_arg) in clean_str(str1_arg):\n",
    "        partial_match = True\n",
    "    \n",
    "    return partial_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "URLSTART = \"http://lyrics.wikia.com/wiki/\"\n",
    "num_playlist = len(playlists_df)\n",
    "\n",
    "for playlist_num in range(num_playlist):\n",
    "    artist_name = playlists_df.iloc[playlist_num]['artist_name']\n",
    "    artist_name_cleaned = re.sub('[^A-Za-z0-9 ]', '', artist_name)\n",
    "    ARTIST = \"_\".join(artist_name.split(' '))\n",
    "    \n",
    "    track_name = playlists_df.iloc[playlist_num]['track_name']\n",
    "    track_name_wo_featuring = track_name.replace('(','-').split('-')[0].strip()\n",
    "    track_name_cleaned = re.sub('[^A-Za-z0-9 ]', '', track_name)\n",
    "    TRACK = get_track_name(track_name)\n",
    "    \n",
    "    album_name = playlists_df.iloc[playlist_num]['album_name']\n",
    "    album_name_cleaned = re.sub('[^A-Za-z0-9 ]', '', album_name)\n",
    "    \n",
    "    \n",
    "    # METHOD 1 - GENIUS\n",
    "    \n",
    "    # Search for matches in the request response\n",
    "   \n",
    "    remote_song_info = None\n",
    "    \n",
    "    for trial in range(4):\n",
    "        # 0 & 1 - search with track name & artist name\n",
    "        # 2 - search with only track name\n",
    "        # 3 - search with only album name\n",
    "        \n",
    "        for try_idx in range(6):\n",
    "            if not(remote_song_info):\n",
    "                # search agin with slightly modified track name and artist name\n",
    "                if try_idx == 0:\n",
    "                    response = request_song_info(track_name_cleaned, artist_name, album_name_cleaned, Genius_TOKEN, trial-2)\n",
    "                elif try_idx == 1:\n",
    "                    response = request_song_info(track_name_wo_featuring, artist_name, album_name_cleaned, Genius_TOKEN, trial-2)\n",
    "                elif try_idx == 2:\n",
    "                    response = request_song_info(track_name_cleaned, artist_name_cleaned, album_name_cleaned, Genius_TOKEN, trial-2)\n",
    "                elif try_idx == 3:\n",
    "                    response = request_song_info(track_name_wo_featuring, artist_name_cleaned, album_name_cleaned, Genius_TOKEN, trial-2)\n",
    "                elif try_idx == 4:\n",
    "                    response = request_song_info(track_name, artist_name, album_name_cleaned, Genius_TOKEN, trial-2)\n",
    "                else:\n",
    "                    response = request_song_info(track_name, artist_name_cleaned, album_name_cleaned, Genius_TOKEN, trial-2)\n",
    "\n",
    "                time.sleep(0.8)\n",
    "            else:\n",
    "                # already found the correct match\n",
    "                break\n",
    "\n",
    "            json = response.json()\n",
    "#             print()\n",
    "#             print(json['response']['hits'])\n",
    "#             print()\n",
    "\n",
    "            for hit in json['response']['hits']:\n",
    "                if partial_match(track_name_wo_featuring, hit['result']['title']):\n",
    "                    # clean_str(track_name_wo_featuring) == clean_str(hit['result']['title'].replace('(','-').split('-')[0]):\n",
    "                    # track_name_cleaned in hit['result']['title'].lower():\n",
    "                    \n",
    "                    if trial == 0:\n",
    "                        # Check the match from both track and artist name\n",
    "                        \n",
    "                        if partial_match(artist_name, hit['result']['primary_artist']['name']):\n",
    "                            # clean_str(artist_name) == clean_str(hit['result']['primary_artist']['name']): \n",
    "                            # artist_name.lower() in hit['result']['primary_artist']['name'].lower():\n",
    "                            \n",
    "                            remote_song_info = hit\n",
    "                            break\n",
    "                    else:\n",
    "                        # Check the match only from track name\n",
    "                        remote_song_info = hit\n",
    "                        break\n",
    "            \n",
    "            if (remote_song_info):\n",
    "                # skip the further search (trial)\n",
    "                break\n",
    "        \n",
    "        if (remote_song_info):\n",
    "            # skip the further search (try_idx)\n",
    "            break\n",
    "    \n",
    "    \n",
    "    # Extract lyrics from URL if the song was found\n",
    "    if remote_song_info:\n",
    "        # there is a match\n",
    "        song_url = remote_song_info['result']['url']\n",
    "        is_genius = True\n",
    "#         lyrics = scrap_song_url(song_url)\n",
    "        \n",
    "    else:\n",
    "        # lyrics not found in GENIUS\n",
    "        \n",
    "        # METHOD 2 - LYRICS.WIKI\n",
    "        song_url = URLSTART + ARTIST + \":\" + TRACK\n",
    "#         song_url = URLSTART + 'None'\n",
    "        is_genius = False\n",
    "        \n",
    "    \n",
    "    page = requests.get(song_url)\n",
    "    \n",
    "    if page.status_code != 200:\n",
    "        audio_feature = sp.audio_features(playlists_df.iloc[playlist_num]['track_uri'])[0]\n",
    "        if len(sp.audio_features(playlists_df.iloc[playlist_num]['track_uri'])) > 1:\n",
    "            print('many audio features')\n",
    "            print('\\nPlaylist # : ', playlist_num)\n",
    "        \n",
    "        if audio_feature['instrumentalness'] > 0.4: # 0.5\n",
    "            # instrumental track\n",
    "            playlists_df.at[playlist_num, 'lyric'] = 'No lyrics'\n",
    "        else:\n",
    "            # Missing lyric in the database\n",
    "\n",
    "            print('\\nPlaylist # : ', playlist_num)\n",
    "            print(\"Status_code :\", page.status_code)\n",
    "            print(f'ARTIST : {artist_name} -> {ARTIST}')\n",
    "            print(f'TRACK : {track_name} -> {TRACK}')\n",
    "            print(f'ALBUM : {album_name}')\n",
    "            print('URL : ', song_url)\n",
    "            print('Track URI : ', playlists_df.iloc[playlist_num]['track_uri'])\n",
    "        \n",
    "        playlists_df.at[playlist_num, 'trial'] = -1\n",
    "        playlists_df.at[playlist_num, 'try_idx'] = -1\n",
    "    else:\n",
    "        # Find the correct lyric\n",
    "        \n",
    "#         print('URL : ', song_url)\n",
    "        \n",
    "#         print(\"1. page.text :\", page.text[0:100],\"...\\n\")\n",
    "#         print(\"2. type(page.text) :\", type(page.text), \"\\n\")\n",
    "#         print(\"3. page.content :\", page.content[0:100],\"...\", \"\\n\")\n",
    "#         print(\"4. type(page.content) :\", type(page.content), \"\\n\") \n",
    "\n",
    "        lyric_soup = BeautifulSoup(page.text,'html.parser')\n",
    "#         print(lyric_soup.prettify()[:])\n",
    "        \n",
    "        lyric = parse_lyric_page(lyric_soup, is_genius)\n",
    "#         print(lyric)\n",
    "        playlists_df.at[playlist_num, 'lyric'] = lyric\n",
    "        playlists_df.at[playlist_num, 'is_genius'] = is_genius\n",
    "        playlists_df.at[playlist_num, 'trial'] = trial\n",
    "        playlists_df.at[playlist_num, 'try_idx'] = try_idx\n",
    "        \n",
    "    out = (playlist_num+1) * 1. / num_playlist * 100\n",
    "    sys.stdout.write(\"\\r %.1f %%\" % out)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    time.sleep(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(playlists_df.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Get various features (e.g. bpm, genre, year, etc.) of each track (using Spotify API) - https://spotipy.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetBPMSoup(ind):\n",
    "    artist = playlists_df.iloc[ind]['artist_name']\n",
    "    track = playlists_df.iloc[ind]['track_name']\n",
    "\n",
    "    artist_clean = artist.split('(')[0].split('!')[0].split('/')[0].split('-')[0].split(',')[0].split(':')[0].split('[')[0].rstrip()\n",
    "    track_clean = track.split('(')[0].split('!')[0].split('/')[0].split('-')[0].split(',')[0].split(':')[0].split('[')[0].rstrip()\n",
    "    \n",
    "    artist_clean = artist_clean.replace('&', '%26').replace('é', 'e').replace('ë', 'e').replace('è', 'e').replace('ü', 'u').replace(' ', '+')\n",
    "    track_clean = track_clean.replace('&', '%26').replace('é', 'e').replace('ë', 'e').replace('è', 'e').replace('ü', 'u').replace(' ', '+')\n",
    "    \n",
    "    url_start = \"https://www.bpmdatabase.com/music/search/?\"\n",
    "    url = url_start + \"artist=\" + artist_clean + \"&title=\" + track_clean\n",
    "    page = requests.get(url)\n",
    "    \n",
    "#     print(url)\n",
    "    return BeautifulSoup(page.text,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetBPMData(soup):\n",
    "    bpm_list = soup.select(\"td[class='bpm']\")\n",
    "    year_list = soup.select(\"td[class='year']\")\n",
    "    genre_list = soup.select(\"td[class='genre']\")\n",
    "    \n",
    "    bpm_contents = np.array([bpm.contents[0] for bpm in bpm_list])\n",
    "    year_contents = np.array([year.contents[0] for year in year_list])\n",
    "    genre_contents = np.array([genre.contents[0] for genre in genre_list])\n",
    "        \n",
    "    bpm_clean = bpm_contents[bpm_contents!='—'].astype('int')\n",
    "    year_clean = year_contents[year_contents!='—'].astype('int')\n",
    "    genre_clean = genre_contents[genre_contents!='—']\n",
    "    \n",
    "    if bpm_clean.size==0:\n",
    "        bpm = np.nan\n",
    "    else:\n",
    "        # bpm: most frequent bpm\n",
    "        uni, ind = np.unique(bpm_clean, return_inverse=True)\n",
    "        bpm =  uni[np.argmax(np.bincount(ind))]\n",
    "        \n",
    "    if year_clean.size==0:\n",
    "        year = np.nan\n",
    "    else:\n",
    "        # year: most frequent year\n",
    "        uni, ind = np.unique(year_clean, return_inverse=True)\n",
    "        year =  uni[np.argmax(np.bincount(ind))]\n",
    "        \n",
    "    if genre_clean.size==0:\n",
    "        genre = []\n",
    "    else:\n",
    "        # genre: all unique strings\n",
    "        genre = np.unique(genre_clean)\n",
    "\n",
    "    return bpm, year, genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddBPMData(df, ind, bpm, year, genre):\n",
    "    df.at[ind, 'bpm'] = bpm\n",
    "    df.at[ind, 'year'] = year\n",
    "    \n",
    "    genre_list = ['2-Step', 'AlternRock', 'Alternative', 'Breaks', 'Cha-Cha', 'Classic Rock', 'Country', 'Dance', 'Electronica',\n",
    "                  'Hard Rock', 'Hip-Hop', 'House', 'Latin', 'Pop', 'Pop-Folk', 'Progressive Rock', 'R&B', 'Reggae', 'Reggaeton',\n",
    "                  'Rock', 'Salsa', 'Top 40', 'Trip-Hop', 'Urban']\n",
    "    \n",
    "    for gen in genre_list:\n",
    "        if gen in genre:\n",
    "             df.at[ind, gen] = int(1)\n",
    "        else:\n",
    "             df.at[ind, gen] = int(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(playlists_df)):\n",
    "#     soup = GetBPMSoup(i)\n",
    "#     bpm, year, genre = GetBPMData(soup)\n",
    "# #     print(i, bpm, year, genre)\n",
    "#     AddBPMData(playlists_df, i, bpm, year, genre)\n",
    "    \n",
    "#     out = (i+1) * 1. / len(playlists_df) * 100\n",
    "#     sys.stdout.write(\"\\r %.1f %%\" % out)\n",
    "#     sys.stdout.flush()\n",
    "    \n",
    "#     time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddAudioFeatures(df, ind, sp):\n",
    "    audio_feature = sp.audio_features(df.iloc[ind]['track_uri'])[0]  \n",
    "    feature_list = ['danceability','energy','key','loudness','mode','speechiness','acousticness',\n",
    "                    'instrumentalness','liveness','valence','tempo','time_signature']\n",
    "    \n",
    "    for feature in feature_list:\n",
    "        df.at[ind, feature] = audio_feature[feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddTrackPopularity(df, ind, sp):\n",
    "    track_info = sp.track(df.iloc[ind]['track_uri'])\n",
    "    df.at[ind, 'popularity'] = track_info['popularity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddArtistGenres(df, ind, sp):\n",
    "    artist_info = sp.artist(df.iloc[ind]['artist_uri'])\n",
    "    genre_list = sp.recommendation_genre_seeds()['genres']\n",
    "    \n",
    "    for gen in genre_list:\n",
    "        if gen in artist_info['genres']:\n",
    "            df.at[ind, gen] = 1\n",
    "        else:\n",
    "            df.at[ind, gen] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddAlbumYear(df, ind, sp):\n",
    "    album_info = sp.album(df.iloc[ind]['album_uri'])\n",
    "    df.at[ind, 'year'] = int(album_info['release_date'].split('-')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(playlists_df)):\n",
    "    AddAlbumYear(playlists_df, i, sp)\n",
    "    AddAudioFeatures(playlists_df, i, sp)\n",
    "    AddTrackPopularity(playlists_df, i, sp)\n",
    "    AddArtistGenres(playlists_df, i, sp)\n",
    "    \n",
    "    out = (i+1) * 1. / len(playlists_df) * 100\n",
    "    sys.stdout.write(\"\\r %.1f %%\" % out)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe for future use\n",
    "playlists_df.to_pickle('web_scraping_df.pkl')\n",
    "# playlists_df.to_pickle('web_scraping_df_lyrics.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load the data from the file\n",
    "playlists_df = pd.read_pickle('web_scraping_df.pkl')\n",
    "\n",
    "print(playlists_df.columns)\n",
    "display(playlists_df.head())\n",
    "# print(np.sum(playlists_df.isnull()))\n",
    "print(f'Shape of playlists_df : {playlists_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_train_df, pl_test_df = train_test_split(playlists_df, test_size=.5, stratify=playlists_df.pid, random_state=99)\n",
    "print(f'Shape of pl_train_df : {pl_train_df.shape}')\n",
    "print(f'Shape of pl_test_df : {pl_test_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. [Feature Engineering]\n",
    "## 1) Generate the simliarity matrix\n",
    "### [1] Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pl_train_df.head())\n",
    "# print('The number of missing values are as follows.')\n",
    "# print(np.sum(pl_train_df.isnull()))\n",
    "# display(pl_train_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_similarity_matrix_str(str_list_arg):\n",
    "    '''\n",
    "    Input\n",
    "        str_list_arg : string feature of the train set (e.g. artist name, album name, track name)\n",
    "    Output\n",
    "        similarity_matrix : element_{i,j} = 1 if string(i) == string(j), 0 otherwise\n",
    "    '''\n",
    "    \n",
    "    ref = np.array([ele.lower() for ele in str_list_arg])\n",
    "    \n",
    "    similarity_matrix = np.zeros([len(str_list_arg), len(str_list_arg)])\n",
    "    \n",
    "    cnt = 0\n",
    "    dc_cnt = 0\n",
    "    \n",
    "    for ele in str_list_arg:\n",
    "        similarity_matrix[cnt] = (ele.lower() == ref)\n",
    "        cnt += 1\n",
    "        dc_cnt += np.sum(ele.lower() == ref)\n",
    "    \n",
    "    return similarity_matrix, dc_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_similarity_matrix_numeric(numeric_list_arg, percentile_arg : float = 0.0):\n",
    "    '''\n",
    "    Input\n",
    "        numeric_list_arg : numeric feature of the train set (e.g. duration (ms), bpm, year)\n",
    "        percentile_arg : when standardize the similarity matrix, use percentile value instead of maximum and minimum\n",
    "                        (e.g. 5% - 95%)\n",
    "    Output\n",
    "        similarity_matrix : element_{i,j} = 1 - standardize(abs(numeric(i) - numeric(j))) \n",
    "                                            (the most similar one has value close to 1, \n",
    "                                            the least similar one has value close to 0)\n",
    "    '''\n",
    "    \n",
    "    ref = numeric_list_arg\n",
    "    \n",
    "    similarity_matrix = np.zeros([len(numeric_list_arg), len(numeric_list_arg)])\n",
    "    \n",
    "    cnt = 0\n",
    "    \n",
    "    for ele in numeric_list_arg:\n",
    "        similarity_matrix[cnt] = np.abs(ref - ele)\n",
    "        cnt += 1\n",
    "    \n",
    "    # NaN imputation - with mean value\n",
    "    mean_value = similarity_matrix[~np.isnan(similarity_matrix)].mean()\n",
    "    similarity_matrix[np.isnan(similarity_matrix)] = mean_value\n",
    "    \n",
    "    # Standardization (0 - 1)\n",
    "    max_ele = np.percentile(similarity_matrix, 100 - percentile_arg)\n",
    "    min_ele = np.percentile(similarity_matrix, percentile_arg)\n",
    "#     max_ele = similarity_matrix.max()\n",
    "#     min_ele = similarity_matrix.min()\n",
    "    \n",
    "#     print('max : ', max_ele)\n",
    "#     print('min : ', min_ele)\n",
    "    similarity_matrix = (similarity_matrix - min_ele) / (max_ele - min_ele)\n",
    "    \n",
    "    similarity_matrix[similarity_matrix > 1] = 1\n",
    "    similarity_matrix[similarity_matrix < 0] = 0\n",
    "    \n",
    "    # Make the most similar one as 1, the least similar one as 0\n",
    "    similarity_matrix = 1 - similarity_matrix\n",
    "    \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_feature_list = ['artist_name', 'album_name', 'track_name']\n",
    "numeric_feature_list = ['duration_ms', 'tempo', 'year']\n",
    "\n",
    "sim_mat = np.zeros([len(pl_train_df), len(pl_train_df), len(str_feature_list) + len(numeric_feature_list)])\n",
    "# sim_mat = np.zeros([len(pl_train_df), len(pl_train_df), len(str_feature_list)])\n",
    "\n",
    "cnt = 0\n",
    "for str_feature in str_feature_list:\n",
    "    sim_mat[:, :, cnt], multiple_str_cnt = make_similarity_matrix_str(pl_train_df[str_feature].values)\n",
    "#     print(multiple_str_cnt)\n",
    "#     print(np.sum(sim_mat[:, :, cnt]))\n",
    "    cnt += 1\n",
    "\n",
    "for numeric_feature in numeric_feature_list:\n",
    "    if numeric_feature != 'duration_ms':\n",
    "        sim_mat[:, :, cnt] = make_similarity_matrix_numeric(pl_train_df[numeric_feature].values)\n",
    "    else:\n",
    "        sim_mat[:, :, cnt] = make_similarity_matrix_numeric(pl_train_df[numeric_feature].values, 5.0)\n",
    "    cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(sim_mat_arg, title: str, ax_arg, show_colormap_arg: bool = True):\n",
    "    '''\n",
    "    input\n",
    "        sim_mat_arg : numpy 2D array (similarity matrix)\n",
    "        title : title of the figure\n",
    "        ax_arg : axis of the figure\n",
    "        show_colormap_arg : if True, show colormap\n",
    "    '''\n",
    "    # Heatmap figure\n",
    "    cbar_axes = sns.heatmap(sim_mat_arg, ax=ax_arg, cbar = show_colormap_arg)\n",
    "\n",
    "    # Update ticklabel size\n",
    "    ax_arg.tick_params(labelsize=20)\n",
    "    ax_arg.set_xticks(np.arange(0, sim_mat_arg.shape[0], 50))\n",
    "    ax_arg.set_xticklabels(np.arange(0, sim_mat_arg.shape[0], 50))\n",
    "    ax_arg.set_xlabel('Index of each track',fontsize = 25)\n",
    "    ax_arg.set_yticks(np.arange(0, sim_mat_arg.shape[0], 50))\n",
    "    ax_arg.set_yticklabels(np.arange(0, sim_mat_arg.shape[0], 50))\n",
    "    ax_arg.set_ylabel('Index of each track',fontsize = 25)\n",
    "\n",
    "    # Update the fontsize of colormapbar\n",
    "    cbar_axes.figure.axes[-1].tick_params(labelsize=25)\n",
    "\n",
    "    # Make labels\n",
    "    ax_arg.set_title(title, fontsize=30)\n",
    "#     ax_arg.grid(True, lw = 0.75, ls = '--', alpha = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_list = [\"Name of artist\", \"Name of album\", \"Name of track\", \"Duration (ms) of track\", \n",
    "                \"Tempo (bpm) of track\", \"Year of track\"]\n",
    "\n",
    "# feature_list = [\"[Similarity matrix] Name of artist (training set)\", \"Name of album (training set)\", 'Name of track (training set)', \n",
    "#                 'Duration of track (training set)', 'Bpm of track (training set)', 'Year of track (training set)']\n",
    "\n",
    "fig, ax = plt.subplots(len(feature_list), 1, figsize=(16, 90))\n",
    "\n",
    "for cnt in range(len(feature_list)):\n",
    "    ax[cnt].set_aspect(1)\n",
    "    plot_heatmap(sim_mat[:,:,cnt], '[Similarity matrix] ' + feature_list[cnt] + ' (training set)', ax[cnt])\n",
    "#     if cnt == 1:\n",
    "#         plot_heatmap(sim_mat[:,:,cnt], feature_list[cnt], ax[cnt//2, cnt%2], True)\n",
    "#     else:\n",
    "#         plot_heatmap(sim_mat[:,:,cnt], feature_list[cnt], ax[cnt//2, cnt%2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Notes]**\n",
    "- All these similarity matrices are from the randomized order of track. <br>\n",
    "- Thus, each playlist is splitted to random index of these matrices. <br>\n",
    "- If we can plot similarity matrix with non-randomized order of track, we might be able to see the chunk of patterns for submatrix in the diagonal, because all the tracks from the same playlist will be placed adjacently. <br>\n",
    "- Let's draw this with full data set. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2] Full data set (Training + Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mat_full = np.zeros([len(playlists_df), len(playlists_df), len(str_feature_list) + len(numeric_feature_list)])\n",
    "\n",
    "cnt = 0\n",
    "for str_feature in str_feature_list:\n",
    "    sim_mat_full[:, :, cnt], multiple_str_cnt = make_similarity_matrix_str(playlists_df[str_feature].values)\n",
    "    cnt += 1\n",
    "\n",
    "for numeric_feature in numeric_feature_list:\n",
    "    if numeric_feature != 'duration_ms':\n",
    "        sim_mat_full[:, :, cnt] = make_similarity_matrix_numeric(playlists_df[numeric_feature].values)\n",
    "    else:\n",
    "        sim_mat_full[:, :, cnt] = make_similarity_matrix_numeric(playlists_df[numeric_feature].values, 5.0)\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(feature_list), 1, figsize=(16, 90))\n",
    "\n",
    "for cnt in range(len(feature_list)):\n",
    "    ax[cnt].set_aspect(1)\n",
    "    plot_heatmap(sim_mat_full[:,:,cnt], '[Similarity matrix] ' + feature_list[cnt] + ' (full data set)', ax[cnt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Notes]**\n",
    "- In this time, we can see some patterns in the similarity matrix. <br>\n",
    "- This is from the full data set (training + test set). <br>\n",
    "- You can see that the chunk of tracks from the same playlist have the same (or similar) features. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1] Global properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_lists = [\"year\", \"danceability\", \"energy\", \"key\", \"loudness\", \"mode\", \"speechiness\", \"acousticness\", \n",
    "                \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"time_signature\", \"popularity\"]\n",
    "\n",
    "genre_lists = sp.recommendation_genre_seeds()['genres']\n",
    "\n",
    "print(\"The list of genre is as follows : \\n\")\n",
    "print(genre_lists)\n",
    "\n",
    "# genre_lists = ['acoustic', 'afrobeat', 'alt-rock', 'alternative', 'ambient', 'anime', 'black-metal', 'bluegrass', \n",
    "#                'blues', 'bossanova', 'brazil', 'breakbeat', 'british', 'cantopop', 'chicago-house', 'children', \n",
    "#                'chill', 'classical', 'club', 'comedy', 'country', 'dance', 'dancehall', 'death-metal', 'deep-house', \n",
    "#                'detroit-techno', 'disco', 'disney', 'drum-and-bass', 'dub', 'dubstep', 'edm', 'electro', \n",
    "#                'electronic', 'emo', 'folk', 'forro', 'french', 'funk', 'garage', 'german', 'gospel', 'goth', \n",
    "#                'grindcore', 'groove', 'grunge', 'guitar', 'happy', 'hard-rock', 'hardcore', 'hardstyle', \n",
    "#                'heavy-metal', 'hip-hop', 'holidays', 'honky-tonk', 'house', 'idm', 'indian', 'indie', \n",
    "#                'indie-pop', 'industrial', 'iranian', 'j-dance', 'j-idol', 'j-pop', 'j-rock', 'jazz', 'k-pop', \n",
    "#                'kids', 'latin', 'latino', 'malay', 'mandopop', 'metal', 'metal-misc', 'metalcore', 'minimal-techno', \n",
    "#                'movies', 'mpb', 'new-age', 'new-release', 'opera', 'pagode', 'party', 'philippines-opm', 'piano', \n",
    "#                'pop', 'pop-film', 'post-dubstep', 'power-pop', 'progressive-house', 'psych-rock', 'punk', \n",
    "#                'punk-rock', 'r-n-b', 'rainy-day', 'reggae', 'reggaeton', 'road-trip', 'rock', 'rock-n-roll', \n",
    "#                'rockabilly', 'romance', 'sad', 'salsa', 'samba', 'sertanejo', 'show-tunes', 'singer-songwriter', \n",
    "#                'ska', 'sleep', 'songwriter', 'soul', 'soundtracks', 'spanish', 'study', 'summer', 'swedish', \n",
    "#                'synth-pop', 'tango', 'techno', 'trance', 'trip-hop', 'turkish', 'work-out', 'world-music']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_histogram(ax_arg, feature_arg, label_arg : str, xlabel_arg : str, color_arg : str = 'b', \n",
    "                   show_ylabel_arg : bool = False, show_legend_arg : bool = False):\n",
    "    '''\n",
    "    Input\n",
    "        ax_arg : axis of the plot\n",
    "        feature_arg : feature data of which we will draw distribution\n",
    "        label_arg : label of the plot\n",
    "        xlabel_arg : xlabel of the plot\n",
    "        color_arg : color of the plot\n",
    "        show_ylabel_arg : if true, show ylabel\n",
    "        show_legend_arg : if true, show legend\n",
    "    '''\n",
    "    if (show_legend_arg):\n",
    "        ax_arg.hist(feature_arg, color = color_arg, alpha = 0.25, label = label_arg)\n",
    "    else:\n",
    "        ax_arg.hist(feature_arg, color = color_arg, alpha = 0.25)\n",
    "    \n",
    "    # Update ticklabel size\n",
    "    ax_arg.tick_params(labelsize = 20)\n",
    "\n",
    "    # Make labels\n",
    "#     ax.set_title('', fontsize = 25)\n",
    "    ax_arg.set_xlabel(xlabel_arg, fontsize = 25)\n",
    "    if (show_ylabel_arg):\n",
    "        ax_arg.set_ylabel('Distribution', fontsize = 25)\n",
    "    if (show_legend_arg):\n",
    "        ax_arg.legend(bbox_to_anchor=(1, 1),loc='upper left', ncol = 1, fontsize = 25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(7, 2, figsize = (16, 55))\n",
    "plt.subplots_adjust(hspace = 0.25)\n",
    "\n",
    "cnt = 0\n",
    "for feature_ele in feature_lists:\n",
    "    if cnt%2 == 0:\n",
    "        show_ylabel = True\n",
    "    else:\n",
    "        show_ylabel = False\n",
    "    \n",
    "    if (cnt%2 == 1) and (cnt//2 ==0):\n",
    "        show_legend = True\n",
    "    else:\n",
    "        show_legend = False\n",
    "    \n",
    "    draw_histogram(ax[cnt//2, cnt%2], pl_train_df[feature_lists[cnt]], 'training set', feature_lists[cnt],\n",
    "                   'b', show_ylabel, show_legend)\n",
    "    draw_histogram(ax[cnt//2, cnt%2], pl_test_df[feature_lists[cnt]], 'test set', feature_lists[cnt],\n",
    "                   'r', show_ylabel, show_legend)\n",
    "    cnt +=1\n",
    "\n",
    "fig.suptitle('Feature distribution in the training and test set', fontsize=30, y = 0.895);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_inter_dependencies(feature_arg, title_arg : str):\n",
    "    '''\n",
    "    Input\n",
    "        feature_arg : feature data of which we will draw distribution\n",
    "        title_arg : title of the plot\n",
    "    '''\n",
    "    ax = scatter_matrix(feature_arg, alpha=0.2, figsize=(18,20), diagonal='kde', range_padding = 0.25)\n",
    "\n",
    "    for aa in np.arange(ax.shape[0]):\n",
    "        for bb in np.arange(ax.shape[1]):\n",
    "            ax[aa,bb].tick_params(labelsize=12)\n",
    "            ax[aa,bb].tick_params(axis='both', labelrotation=45)\n",
    "            ax[aa,bb].xaxis.label.set_size(15)\n",
    "            ax[aa,bb].yaxis.label.set_size(15)\n",
    "            \n",
    "            if (bb % 2 == 0):\n",
    "    #             ax[aa,bb].xaxis.label.set_va('top')\n",
    "                ax[aa,bb].xaxis.set_label_coords(0.5,-0.5)\n",
    "            else:\n",
    "    #             ax[aa,bb].xaxis.label.set_va('bottom')\n",
    "                ax[aa,bb].xaxis.set_label_coords(0.5,-0.75)\n",
    "            if (aa % 2 == 0):\n",
    "    #             ax[aa,bb].yaxis.label.set_va('top')\n",
    "                ax[aa,bb].yaxis.set_label_coords(-0.75,0.5)\n",
    "            else:\n",
    "    #             ax[aa,bb].yaxis.label.set_va('bottom')\n",
    "                ax[aa,bb].yaxis.set_label_coords(-1.0,0.5)\n",
    "\n",
    "    plt.suptitle(title_arg, fontsize=25,y=0.9);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_inter_dependencies(pl_train_df[feature_lists], '[Training set] Inter-dependencies among all predictors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_inter_dependencies(pl_test_df[feature_lists], '[Test set] Inter-dependencies among all predictors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nonzero_genre_temp = (np.sum(playlists_df[genre_lists]) != 0)\n",
    "nonzero_genre = [ele for ele in nonzero_genre_temp[nonzero_genre_temp.values].index]\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (16, 8))\n",
    "\n",
    "ind = np.arange(len(nonzero_genre)) # the x locations for the groups\n",
    "width = 0.4         # the width of the bars\n",
    "ax.bar(ind-width/2, pl_train_df[nonzero_genre].sum().values, width, color='b', label = 'traning set')\n",
    "ax.bar(ind+width/2, pl_test_df[nonzero_genre].sum().values, width, color='r', label = 'test set')\n",
    "\n",
    "# Update ticklabel size\n",
    "ax.tick_params(labelsize = 20)\n",
    "ax.tick_params(axis='x', rotation = 90)\n",
    "ax.set_xticks(ind)\n",
    "\n",
    "# Make labels\n",
    "ax.set_ylabel('# of tracks',fontsize=25)\n",
    "ax.set_xticklabels(nonzero_genre, fontsize=10)\n",
    "\n",
    "ax.legend(fontsize=20)\n",
    "# ax.tick_params(axis='both', which='both', length=0)\n",
    "ax.grid(True, lw =  1.5, ls = '--', alpha = 0.75)\n",
    "ax.set_title('Distribution of genre of the track', fontsize=30, y = 1.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2] Group properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(10, 2, figsize = (16, 60))\n",
    "plt.subplots_adjust(hspace = 1.0)\n",
    "\n",
    "ind = np.arange(len(nonzero_genre)) # the x locations for the groups\n",
    "width = 0.7         # the width of the bars\n",
    "\n",
    "cnt = 0\n",
    "for pid in playlists_df.groupby('pid').sum()[nonzero_genre].index:\n",
    "#     print(playlists_df.groupby('pid').sum()[nonzero_genre].loc[pid].values)\n",
    "    \n",
    "    ax_temp = ax[cnt//2, cnt%2]\n",
    "    \n",
    "    ax_temp.bar(ind, playlists_df.groupby('pid').sum()[nonzero_genre].loc[pid].values, \n",
    "           width, color='b', label = 'playlist ' + str(pid))\n",
    "    \n",
    "    # Update ticklabel size\n",
    "    ax_temp.tick_params(labelsize = 15)\n",
    "    ax_temp.tick_params(axis='x', rotation = 90)\n",
    "    if cnt == len(nonzero_genre)-1:\n",
    "        ax_temp.set_xticks(ind)\n",
    "    \n",
    "    # Make labels\n",
    "    if cnt%2 == 0:\n",
    "        ax_temp.set_ylabel('# of tracks',fontsize=20)\n",
    "    ax_temp.set_xticks(ind)\n",
    "    ax_temp.set_xticklabels(nonzero_genre, fontsize=10)\n",
    "    \n",
    "    ax_temp.legend(fontsize=15)\n",
    "    ax_temp.grid(True, lw =  1.5, ls = '--', alpha = 0.75)\n",
    "    \n",
    "    cnt += 1\n",
    "\n",
    "fig.suptitle('Distribution of genre of the track (for each playlist)', fontsize=30, y = 0.89);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Notes]**\n",
    "- The above barplot is the distribution of genre of the track for each playlist. <br>\n",
    "As you can see, each playlist is highly dependent on the genre of the track. <br>\n",
    "Large portion of the playlist is consisted of only several genre of the track, meaning that the genre will be pretty important feature to continue the playlist. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Scaler is as follows.')\n",
    "scaler = MinMaxScaler().fit(pl_train_df[feature_lists])\n",
    "display(pd.DataFrame({'min': scaler.min_, 'scale': scaler.scale_, \n",
    "                      'data_min' : scaler.data_min_, 'data_max' : scaler.data_max_}, index = feature_lists).T)\n",
    "# scaler = StandardScaler().fit(practice_X_train[feature_lists])\n",
    "# display(pd.DataFrame({'mean': scaler.mean_, 'variance': scaler.var_}, index = feature_lists).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_columns(df_arg, scaler_arg, col_arg: list):\n",
    "    '''\n",
    "        input\n",
    "            df_arg : data_frame\n",
    "            scaler_arg : StandardScaler object containing the information (min, max) of columns of training set\n",
    "            col_arg : a list of columns\n",
    "        output\n",
    "            scaled_df : standardized data_frame\n",
    "    '''\n",
    "    # copy the dataframe in order not to affect to original data frame \n",
    "    scaled_df = df_arg.copy()\n",
    "    scaled_df[col_arg] = scaler_arg.transform(df_arg[col_arg])\n",
    "    \n",
    "    return scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_pl_train_df = scale_columns(pl_train_df, scaler, feature_lists)\n",
    "scaled_pl_test_df = scale_columns(pl_test_df, scaler, feature_lists)\n",
    "\n",
    "print('1. [Training set]\\n')\n",
    "print('Before Standardization')\n",
    "display(pl_train_df.describe())\n",
    "\n",
    "print('After Standardization')\n",
    "display(scaled_pl_train_df.describe())\n",
    "\n",
    "print('2. [Test set]\\n')\n",
    "print('Before Standardization')\n",
    "display(pl_test_df.describe())\n",
    "\n",
    "print('After Standardization')\n",
    "display(scaled_pl_test_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Multi-dimensional scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_columns = []\n",
    "eff_columns.extend(feature_lists)\n",
    "eff_columns.extend(genre_lists)\n",
    "\n",
    "embedding = MDS(n_components=2, n_jobs = -1)\n",
    "MDS_pl_train = embedding.fit_transform(scaled_pl_train_df[eff_columns])\n",
    "print('[Result of Multi-Dimensional Scaling (MDS)]')\n",
    "print(f'Shape of MDS_pl_train : {MDS_pl_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_MDS_result(MDS_arg, y_train_arg, class_arg):\n",
    "    '''\n",
    "    Input\n",
    "        MDS_arg : 2 dimensional feature spaces\n",
    "        y_train_arg : label of each data point\n",
    "        class_arg : list of class\n",
    "    '''\n",
    "    marker_list = ['.','o','^','^','<','>','1','2','3','4','8','s','p','P','*','+','x','X','D','d']\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize = (12, 12))\n",
    "    \n",
    "    cnt = 0\n",
    "    for class_ele in class_arg:\n",
    "        ax.scatter(MDS_arg[y_train_arg == class_ele, 0], MDS_arg[y_train_arg == class_ele, 1], \n",
    "                   s = 100, alpha = 0.75, edgecolors = 'k', linewidth = 1.5, marker = marker_list[cnt], \n",
    "                   label = 'Playlist ' + str(class_ele))\n",
    "        cnt += 1\n",
    "    \n",
    "    # Update ticklabel size\n",
    "    ax.tick_params(labelsize = 20)\n",
    "    \n",
    "    # Make labels\n",
    "    ax.set_title(f'Visualization of {len(MDS_arg)} tracks in {len(class_arg)} playlists using MDS', fontsize = 30)\n",
    "    \n",
    "    ax.set_xlabel('1st Feature', fontsize = 25)\n",
    "    ax.set_ylabel('2nd Feature', fontsize = 25)\n",
    "    ax.legend(bbox_to_anchor=(1, 1),loc='upper left', ncol = 1, fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_lists = np.unique(scaled_pl_train_df['pid'].values)\n",
    "y_train = scaled_pl_train_df['pid'].values\n",
    "y_test = scaled_pl_test_df['pid'].values\n",
    "\n",
    "plot_MDS_result(MDS_pl_train, y_train, class_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. [Models]\n",
    "## 1) Base model - k nearest neighbors (k-NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaled_pl_train_df[eff_columns]\n",
    "X_test = scaled_pl_test_df[eff_columns]\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_jobs = -1)\n",
    "knn_model.fit(X_train, y_train)\n",
    "print(\"[k-NN result (k=5)]\")\n",
    "print(f\"Training accuracy: {knn_model.score(X_train, y_train) * 100.0:.2f} %\")\n",
    "print(f\"Test accuracy: {knn_model.score(X_test, y_test) * 100.0:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) k-NN with Cross Validation (CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN with 5-fold crossvalidation - with GridSearchCV\n",
    "\n",
    "KFold_k = 5\n",
    "k_list = np.arange(1, 100, 2)\n",
    "\n",
    "est = KNeighborsClassifier(n_jobs = -1)\n",
    "parameters = {'n_neighbors': k_list}\n",
    "gs = GridSearchCV(est, param_grid=parameters, cv=KFold_k, scoring=\"accuracy\")\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(\"[k-NN with CV result]\")\n",
    "print(gs.best_estimator_, \"\\n\")\n",
    "print(gs.best_params_, \"\\n\")\n",
    "print(gs.best_score_, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = gs.cv_results_['mean_test_score'].argmax()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (8,8)) \n",
    "\n",
    "# for fold in range(KFold_k):\n",
    "#     key = 'split' + str(fold) + '_test_score'\n",
    "#     ax.plot(k_list, gs.cv_results_[key], 'b', lw = 1, linestyle = '--', label = f'Fold {fold+1}')\n",
    "\n",
    "\n",
    "ax.fill_between(k_list, gs.cv_results_['mean_test_score'] + gs.cv_results_['std_test_score'], \n",
    "                gs.cv_results_['mean_test_score'] - gs.cv_results_['std_test_score'], color = 'b', alpha = 0.1,\n",
    "                label = f'5-Fold Avg $\\pm$ 1 std')\n",
    "ax.plot(k_list, gs.cv_results_['mean_test_score'], '*-', color = 'b', linewidth = 3, markersize = 12, \n",
    "        alpha = 0.5, label = 'Mean 5-CV')\n",
    "\n",
    "# ax.plot(k_list, gs.cv_results_['mean_test_score'], 'r', lw = 3, label = '5-Fold Avg')\n",
    "# ax.plot(k_list, gs.cv_results_['mean_test_score'] + gs.cv_results_['std_test_score'], 'r', lw = 1, linestyle = '--', \n",
    "#         label = f'5-Fold Avg $\\pm$ 1 std')\n",
    "# ax.plot(k_list, gs.cv_results_['mean_test_score'] - gs.cv_results_['std_test_score'], 'r', lw = 1, linestyle = '--')\n",
    "\n",
    "ax.scatter(k_list[best_idx], gs.cv_results_['mean_test_score'][best_idx], color=\"r\", marker=\"*\", s=1000, \n",
    "           label=\"Best model\")\n",
    "\n",
    "# Update ticklabel size\n",
    "ax.tick_params(labelsize = 20)\n",
    "\n",
    "# Make labels\n",
    "ax.set_title('Average accuracy of k-NN classifier [Validation set]\\n', fontsize = 25) \n",
    "ax.set_xlabel('k value', fontsize = 25)\n",
    "ax.set_ylabel('Validation Accuracy', fontsize = 25)\n",
    "ax.legend(bbox_to_anchor=(1, 1),loc='upper left', ncol = 1, fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors = gs.best_params_['n_neighbors'], n_jobs = -1)\n",
    "knn_model.fit(X_train, y_train)\n",
    "print(f\"[k-NN from CV result (k={gs.best_params_['n_neighbors']})]\")\n",
    "print(f\"Training accuracy: {knn_model.score(X_train, y_train) * 100.0:.2f} %\")\n",
    "print(f\"Test accuracy: {knn_model.score(X_test, y_test) * 100.0:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. [Reference]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [1] Berenzweig, Adam, Beth Logan, Daniel P.W. Ellis and Brian Whitman. A Large-Scale Evaluation of Acoustic and Subjective Music Similarity Measures. Proceedings of the ISMIR International Conference on Music Information Retrieval (Baltimore, MD), 2003, pp. 99:105. <br>\n",
    "[2] Logan, B., A Content-Based Music Similarity Function, (Report CRL 2001/02) Compaq Computer Corporation Cambridge Research Laboratory, Technical Report Series (Jun. 2001). <br>\n",
    "[3] Shedl, M. et al, Current Challenges and Visions in Music Recommender Systems Research, https://arxiv.org/pdf/1710.03208.pdf. <br>\n",
    "[4] Shedl, M., Peter Knees, and Fabien Gouyon, New Paths in Music Recommender Systems, RecSys’17 tutorial, http://www.cp.jku.at/tutorials/mrs_recsys_2017/slides.pdf <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
